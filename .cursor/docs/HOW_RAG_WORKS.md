## Как работают RAG-системы в Enterprise

В enterprise-проектах RAG (Retrieval-Augmented Generation) обычно работает по такой схеме:

### 1. **Индексация документов** (оффлайн этап)
```
Документы → Чанкинг → Эмбеддинг → Векторная БД
```
- Документы разбиваются на chunks (чанки)
- Каждый чанк преобразуется в эмбеддинг через `/api/embed`
- Эмбеддинги сохраняются в векторную БД (Pinecone, Weaviate, Chroma и т.д.)

### 2. **Поиск релевантного контекста** (онлайн этап)
```
Пользовательский запрос → Эмбеддинг запроса → Поиск в векторной БД
```
- Запрос пользователя преобразуется в эмбеддинг через `/api/embed`
- В векторной БД ищутся N наиболее похожих чанков

### 3. **Генерация ответа**
```
Запрос + Контекст → LLM → Ответ
```
- Найденные текстовые чанки подставляются в промпт как контекст
- **Отправляется обычный текстовый запрос** в `/api/generate`

### `/api/embed` - только для создания эмбеддингов
```bash
# Запрос
curl http://localhost:11434/api/embed -d '{
  "model": "nomic-embed-text",
  "prompt": "What is RAG?"
}'

# Ответ - только вектор, без генерации текста
{
  "embedding": [0.123, -0.456, 0.789, ...]
}
```

### `/api/generate` - работает с текстом, не с эмбеддингами
```bash
# Запрос - ТОЛЬКО текст, никаких эмбеддингов
curl http://localhost:11434/api/generate -d '{
  "model": "llama2",
  "prompt": "Context: RAG stands for Retrieval-Augmented Generation...\n\nQuestion: What is RAG?",
  "stream": false
}'
```

## Почему так сделано?

1. **Разделение ответственности**:
   - Embedding-модели оптимизированы для создания векторных представлений
   - LLM-модели оптимизированы для генерации текста

2. **Эмбеддинги не передаются в LLM напрямую**:
   - LLM понимают только токены (текст), а не векторы
   - Контекст передается как обычный текст в промпте

3. **Гибкость архитектуры**:
   - Можно использовать разные модели для эмбеддингов и генерации
   - Векторный поиск и генерация - независимые компоненты

## Пример полного RAG пайплайна:

```python
# 1. Создаем эмбеддинг для запроса
query_embedding = ollama.embed(
    model="embedding-model",
    prompt=user_question
)

# 2. Ищем релевантные документы в векторной БД
relevant_chunks = vector_db.similarity_search(query_embedding)

# 3. Формируем промпт с контекстом
context = "\n".join([chunk.text for chunk in relevant_chunks])
prompt = f"""
Context: {context}

Question: {user_question}
Answer:
"""

# 4. Генерируем ответ (БЕЗ передачи эмбеддингов!)
response = ollama.generate(
    model="llama2",
    prompt=prompt
)
```